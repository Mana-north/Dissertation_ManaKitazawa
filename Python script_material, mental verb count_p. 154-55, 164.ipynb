{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e895ac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique word types: 114\n",
      "       Word  Count\n",
      "0   provide     16\n",
      "1     serve      6\n",
      "2   operate      6\n",
      "3      make      6\n",
      "4   deliver      5\n",
      "5      help      4\n",
      "6    enable      4\n",
      "7    market      3\n",
      "8     offer      3\n",
      "9  maintain      3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#★\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure you have the necessary NLTK datasets downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# A helper function to convert NLTK's part of speech tags to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun for unknown cases\n",
    "\n",
    "# Read the list of verbs from a file named '1998_verbs.txt'\n",
    "with open('1998_verbs.txt', 'r') as file:\n",
    "    verbs = file.read().strip().lower().split('\\n')  # Convert to lowercase and split by new lines\n",
    "\n",
    "# Tag each verb with its part of speech\n",
    "tagged_verbs = nltk.pos_tag(verbs)\n",
    "\n",
    "# Lemmatize each word (now including all parts of speech, not just verbs)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_verbs]\n",
    "\n",
    "# Count each lemmatized word's frequency\n",
    "word_frequency = Counter(lemmatized_words)\n",
    "\n",
    "# Convert the counter to a dataframe for better visualization\n",
    "df_words = pd.DataFrame(word_frequency.items(), columns=['Word', 'Count'])\n",
    "\n",
    "# Sort the dataframe by count in descending order to get the ranking\n",
    "df_words.sort_values('Count', ascending=False, inplace=True)\n",
    "df_words.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the total number of unique word types and the top 10 rows\n",
    "print(f\"Total unique word types: {len(df_words)}\")\n",
    "print(df_words.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#★\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure you have the necessary NLTK datasets downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# A helper function to convert NLTK's part of speech tags to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun for unknown cases\n",
    "\n",
    "# Read the list of verbs from a file named '1998_verbs.txt'\n",
    "with open('2005', 'r') as file:\n",
    "    verbs = file.read().strip().lower().split('\\n')  # Convert to lowercase and split by new lines\n",
    "\n",
    "# Tag each verb with its part of speech\n",
    "tagged_verbs = nltk.pos_tag(verbs)\n",
    "\n",
    "# Lemmatize each word (now including all parts of speech, not just verbs)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_verbs]\n",
    "\n",
    "# Count each lemmatized word's frequency\n",
    "word_frequency = Counter(lemmatized_words)\n",
    "\n",
    "# Convert the counter to a dataframe for better visualization\n",
    "df_words = pd.DataFrame(word_frequency.items(), columns=['Word', 'Count'])\n",
    "\n",
    "# Sort the dataframe by count in descending order to get the ranking\n",
    "df_words.sort_values('Count', ascending=False, inplace=True)\n",
    "df_words.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the total number of unique word types and the top 10 rows\n",
    "print(f\"Total unique word types: {len(df_words)}\")\n",
    "print(df_words.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e35efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Verb  Count\n",
      "0   deliver      8\n",
      "1        do      8\n",
      "2    create      6\n",
      "3   provide      6\n",
      "4     serve      5\n",
      "5      make      5\n",
      "6  leverage      5\n",
      "7   achieve      5\n",
      "8      help      4\n",
      "9   empower      4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure you have the necessary NLTK datasets downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# A helper function to convert NLTK's part of speech tags to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if unknown\n",
    "\n",
    "# Read the list of verbs from a file named '1998_verbs.txt'\n",
    "with open('2021_verbs.txt', 'r') as file:\n",
    "    verbs = file.read().strip().lower().split('\\n')  # Convert to lowercase and split by new lines\n",
    "\n",
    "# Lemmatize each verb in the list\n",
    "lemmatized_verbs = [lemmatizer.lemmatize(verb, get_wordnet_pos(pos)) for verb, pos in nltk.pos_tag(verbs)]\n",
    "\n",
    "# Count each lemmatized verb's frequency\n",
    "verb_frequency = Counter(lemmatized_verbs)\n",
    "\n",
    "# Convert the counter to a dataframe for better visualization\n",
    "df_verbs = pd.DataFrame(verb_frequency.most_common(), columns=['Verb', 'Count'])\n",
    "\n",
    "# Sort the dataframe by count in descending order to get the ranking\n",
    "df_verbs.sort_values('Count', ascending=False, inplace=True)\n",
    "df_verbs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the top 10 rows\n",
    "print(df_verbs.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c16c9092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique word types: 92\n",
      "       Word  Count\n",
      "0        do      8\n",
      "1   deliver      8\n",
      "2   provide      6\n",
      "3    create      6\n",
      "4      make      5\n",
      "5     serve      5\n",
      "6  leverage      5\n",
      "7   achieve      5\n",
      "8      help      4\n",
      "9   empower      4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#★\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure you have the necessary NLTK datasets downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# A helper function to convert NLTK's part of speech tags to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun for unknown cases\n",
    "\n",
    "# Read the list of verbs from a file named '1998_verbs.txt'\n",
    "with open('2021_verbs.txt', 'r') as file:\n",
    "    verbs = file.read().strip().lower().split('\\n')  # Convert to lowercase and split by new lines\n",
    "\n",
    "# Tag each verb with its part of speech\n",
    "tagged_verbs = nltk.pos_tag(verbs)\n",
    "\n",
    "# Lemmatize each word (now including all parts of speech, not just verbs)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_verbs]\n",
    "\n",
    "# Count each lemmatized word's frequency\n",
    "word_frequency = Counter(lemmatized_words)\n",
    "\n",
    "# Convert the counter to a dataframe for better visualization\n",
    "df_words = pd.DataFrame(word_frequency.items(), columns=['Word', 'Count'])\n",
    "\n",
    "# Sort the dataframe by count in descending order to get the ranking\n",
    "df_words.sort_values('Count', ascending=False, inplace=True)\n",
    "df_words.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the total number of unique word types and the top 10 rows\n",
    "print(f\"Total unique word types: {len(df_words)}\")\n",
    "print(df_words.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a93fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Verb  Count\n",
      "0         believe      2\n",
      "1           focus      2\n",
      "2       discovers      1\n",
      "3       celebrate      1\n",
      "4        discover      1\n",
      "5   is considered      1\n",
      "6  are focused on      1\n",
      "7            find      1\n",
      "8            want      1\n",
      "9   is recognized      1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure you have the necessary NLTK datasets downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# A helper function to convert NLTK's part of speech tags to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if unknown\n",
    "\n",
    "# Read the list of verbs from a file named '1998_verbs.txt'\n",
    "with open('1998_mentalverb.txt', 'r') as file:\n",
    "    verbs = file.read().strip().lower().split('\\n')  # Convert to lowercase and split by new lines\n",
    "\n",
    "# Lemmatize each verb in the list\n",
    "lemmatized_verbs = [lemmatizer.lemmatize(verb, get_wordnet_pos(pos)) for verb, pos in nltk.pos_tag(verbs)]\n",
    "\n",
    "# Count each lemmatized verb's frequency\n",
    "verb_frequency = Counter(lemmatized_verbs)\n",
    "\n",
    "# Convert the counter to a dataframe for better visualization\n",
    "df_verbs = pd.DataFrame(verb_frequency.most_common(), columns=['Verb', 'Count'])\n",
    "\n",
    "# Sort the dataframe by count in descending order to get the ranking\n",
    "df_verbs.sort_values('Count', ascending=False, inplace=True)\n",
    "df_verbs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the top 10 rows\n",
    "print(df_verbs.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83240709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique word types: 18\n",
      "             Word  Count\n",
      "0           focus      2\n",
      "1         believe      2\n",
      "2          intend      1\n",
      "3            want      1\n",
      "4       discovers      1\n",
      "5       celebrate      1\n",
      "6        discover      1\n",
      "7   is considered      1\n",
      "8  are focused on      1\n",
      "9            find      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#★\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure you have the necessary NLTK datasets downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# A helper function to convert NLTK's part of speech tags to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun for unknown cases\n",
    "\n",
    "# Read the list of verbs from a file named '1998_verbs.txt'\n",
    "with open('1998_mentalverb.txt', 'r') as file:\n",
    "    verbs = file.read().strip().lower().split('\\n')  # Convert to lowercase and split by new lines\n",
    "\n",
    "# Tag each verb with its part of speech\n",
    "tagged_verbs = nltk.pos_tag(verbs)\n",
    "\n",
    "# Lemmatize each word (now including all parts of speech, not just verbs)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_verbs]\n",
    "\n",
    "# Count each lemmatized word's frequency\n",
    "word_frequency = Counter(lemmatized_words)\n",
    "\n",
    "# Convert the counter to a dataframe for better visualization\n",
    "df_words = pd.DataFrame(word_frequency.items(), columns=['Word', 'Count'])\n",
    "\n",
    "# Sort the dataframe by count in descending order to get the ranking\n",
    "df_words.sort_values('Count', ascending=False, inplace=True)\n",
    "df_words.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the total number of unique word types and the top 10 rows\n",
    "print(f\"Total unique word types: {len(df_words)}\")\n",
    "print(df_words.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0056eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Verb  Count\n",
      "0                       see      3\n",
      "1                  may know      2\n",
      "2                    ensure      2\n",
      "3                   believe      1\n",
      "4              committed to      1\n",
      "5                    expect      1\n",
      "6                      find      1\n",
      "7  are relentlessly focused      1\n",
      "8                  envision      1\n",
      "9      can easily determine      1\n"
     ]
    }
   ],
   "source": [
    "# Read the list of verbs from a file named '1998_verbs.txt'\n",
    "with open('2005_mentalverb.txt', 'r') as file:\n",
    "    verbs = file.read().strip().lower().split('\\n')  # Convert to lowercase and split by new lines\n",
    "\n",
    "# Lemmatize each verb in the list\n",
    "lemmatized_verbs = [lemmatizer.lemmatize(verb, get_wordnet_pos(pos)) for verb, pos in nltk.pos_tag(verbs)]\n",
    "\n",
    "# Count each lemmatized verb's frequency\n",
    "verb_frequency = Counter(lemmatized_verbs)\n",
    "\n",
    "# Convert the counter to a dataframe for better visualization\n",
    "df_verbs = pd.DataFrame(verb_frequency.most_common(), columns=['Verb', 'Count'])\n",
    "\n",
    "# Sort the dataframe by count in descending order to get the ranking\n",
    "df_verbs.sort_values('Count', ascending=False, inplace=True)\n",
    "df_verbs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the top 10 rows\n",
    "print(df_verbs.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83784117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique word types: 15\n",
      "                       Word  Count\n",
      "0                       see      3\n",
      "1                  may know      2\n",
      "2                    ensure      2\n",
      "3                   believe      1\n",
      "4              committed to      1\n",
      "5                    expect      1\n",
      "6                      find      1\n",
      "7  are relentlessly focused      1\n",
      "8                  envision      1\n",
      "9      can easily determine      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#★\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure you have the necessary NLTK datasets downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# A helper function to convert NLTK's part of speech tags to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun for unknown cases\n",
    "\n",
    "# Read the list of verbs from a file named '1998_verbs.txt'\n",
    "with open('2005_mentalverb.txt', 'r') as file:\n",
    "    verbs = file.read().strip().lower().split('\\n')  # Convert to lowercase and split by new lines\n",
    "\n",
    "# Tag each verb with its part of speech\n",
    "tagged_verbs = nltk.pos_tag(verbs)\n",
    "\n",
    "# Lemmatize each word (now including all parts of speech, not just verbs)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_verbs]\n",
    "\n",
    "# Count each lemmatized word's frequency\n",
    "word_frequency = Counter(lemmatized_words)\n",
    "\n",
    "# Convert the counter to a dataframe for better visualization\n",
    "df_words = pd.DataFrame(word_frequency.items(), columns=['Word', 'Count'])\n",
    "\n",
    "# Sort the dataframe by count in descending order to get the ranking\n",
    "df_words.sort_values('Count', ascending=False, inplace=True)\n",
    "df_words.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the total number of unique word types and the top 10 rows\n",
    "print(f\"Total unique word types: {len(df_words)}\")\n",
    "print(df_words.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5abd4594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Verb  Count\n",
      "0       value      2\n",
      "1  prioritize      2\n",
      "2     embrace      2\n",
      "3       focus      1\n",
      "4        look      1\n",
      "5    evaluate      1\n",
      "6   challenge      1\n",
      "7  anticipate      1\n",
      "8    focus on      1\n",
      "9    discover      1\n"
     ]
    }
   ],
   "source": [
    "# Read the list of verbs from a file named '1998_verbs.txt'\n",
    "with open('2021_mentalverb.txt', 'r') as file:\n",
    "    verbs = file.read().strip().lower().split('\\n')  # Convert to lowercase and split by new lines\n",
    "\n",
    "# Lemmatize each verb in the list\n",
    "lemmatized_verbs = [lemmatizer.lemmatize(verb, get_wordnet_pos(pos)) for verb, pos in nltk.pos_tag(verbs)]\n",
    "\n",
    "# Count each lemmatized verb's frequency\n",
    "verb_frequency = Counter(lemmatized_verbs)\n",
    "\n",
    "# Convert the counter to a dataframe for better visualization\n",
    "df_verbs = pd.DataFrame(verb_frequency.most_common(), columns=['Verb', 'Count'])\n",
    "\n",
    "# Sort the dataframe by count in descending order to get the ranking\n",
    "df_verbs.sort_values('Count', ascending=False, inplace=True)\n",
    "df_verbs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the top 10 rows\n",
    "print(df_verbs.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa7fc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique word types: 25\n",
      "         Word  Count\n",
      "0       value      2\n",
      "1  prioritize      2\n",
      "2     embrace      2\n",
      "3       focus      1\n",
      "4        look      1\n",
      "5    evaluate      1\n",
      "6   challenge      1\n",
      "7  anticipate      1\n",
      "8    focus on      1\n",
      "9    discover      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kitaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#★\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure you have the necessary NLTK datasets downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# A helper function to convert NLTK's part of speech tags to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun for unknown cases\n",
    "\n",
    "# Read the list of verbs from a file named '1998_verbs.txt'\n",
    "with open('2021_mentalverb.txt', 'r') as file:\n",
    "    verbs = file.read().strip().lower().split('\\n')  # Convert to lowercase and split by new lines\n",
    "\n",
    "# Tag each verb with its part of speech\n",
    "tagged_verbs = nltk.pos_tag(verbs)\n",
    "\n",
    "# Lemmatize each word (now including all parts of speech, not just verbs)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_verbs]\n",
    "\n",
    "# Count each lemmatized word's frequency\n",
    "word_frequency = Counter(lemmatized_words)\n",
    "\n",
    "# Convert the counter to a dataframe for better visualization\n",
    "df_words = pd.DataFrame(word_frequency.items(), columns=['Word', 'Count'])\n",
    "\n",
    "# Sort the dataframe by count in descending order to get the ranking\n",
    "df_words.sort_values('Count', ascending=False, inplace=True)\n",
    "df_words.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the total number of unique word types and the top 10 rows\n",
    "print(f\"Total unique word types: {len(df_words)}\")\n",
    "print(df_words.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4035fd8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
